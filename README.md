# JAMO (ìëª¨): Korean LLM (Little Language Model) 

<img src="./assets/parrot.png" style="max-width:400px;align-items: center;">

*"It's just a simple stochastic parrots"* -- by Dall-E 2

JAMO(ìëª¨)ëŠ” ì„¸ë§ˆê³ ë“±í•™êµ R&E ì—°êµ¬ë¥¼ í•˜ê³  ìˆëŠ” í•™ìƒì´ ê°œë°œí•œ í•œêµ­ì–´ Large Language Model ì…ë‹ˆë‹¤.

ìëª¨ í”„ë¡œì íŠ¸ëŠ” ê¸°ë³¸ì˜ GPT-3ë‚˜ GPT-4ì™€ ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ **ë”** ì¤„ì´ê¸° ìœ„í•œ í”„ë¡œì íŠ¸ì˜ ì¼í™˜ìœ¼ë¡œ ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œë„ í’€ëª¨ë¸ êµ¬ë™ì´ ê°€ëŠ¥í•˜ë„ë¡ ì—°êµ¬ë¥¼ ì§„í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤. 


### Install

**ê¹ƒí—ˆë¸Œ ë ˆíŒŒì§€í† ë¦¬ í´ë¡ **

```bash
git clone https://github.com/yoonhero/jamo_llm
```

**íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ**

```bash
pip install -r requirements.txt
```

ì„¸íŒ…ì´ ì™„ë£Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰


### Use the model


### Pretraining with A100

ìëª¨ëŠ” í•œêµ­ì–´ ëª¨ë¸ë¡œ [nanoGPT](https://github.com/karpathy/nanoGPT)ì— ê¸°ë°˜ì„ ë‘” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. 
GPT ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ , ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì±„íƒí•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. 

- ìëª¨ì˜ ì‚¬ì „ í•™ìŠµì€ GIST [Sundong Kim](https://sundong.kim/) êµìˆ˜ë‹˜ì˜ A100 ì§€ì›ìœ¼ë¡œ í•™ìŠµë  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

ìëª¨ëŠ” ì•½ 12ì¼ 300000 epoch ë™ì•ˆ 64,512,000,000ê°œì˜ í† í°ì„ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤. 
ë³¸ ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ì…‹ì€ ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. 



**Run Pretrain On Your Device**

ëª¨ë¸ ìì²´ì˜ í¬ê¸°ëŠ” í¬ì§€ ì•Šì•„ì„œ GPUì— ë¡œë”©í•  ìˆ˜ ìˆê² ì§€ë§Œ ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ì„œ í° VRAM í™˜ê²½ì—ì„œ í•™ìŠµí•˜ê¸°ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.

```bash
cd pretrain
python pretrain.py \
    --model_size "small" \
	--learning_rate 0.0003 \ 
	--min_lr 0.00005 \
	--batch_size 70 \ 
	--max_iters 100000 \
	--warmup_iters 2000 \
	--save_interval 5000 \
	--eval_interval 500 \ 
	--gradient_accumulate 6 \
	--checkpoint_dir "../tmp/checkpoint" \ 
	--corpus_path "../tmp/dataset.txt" \
	--tokenizer_path "hg_tokenizer" \       
	--with_lr_scheduler 
```


### Fine-Tune Model



### Citation

Please cite the repo if you use the data or code in this repo.

```
@misc{jamo,
  author = {Sema Highschool LLM R&E Research Team},
  title = {JAMO LLM: JAMO Little Language Model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yoonhero/jamo}},
}
```

### Acknowledgments

We gratefully acknowledge support from:

```
@article{liu2023sophia,
 title={Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training},
 author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
 journal={arXiv preprint arXiv:2305.14342},
 year={2023}
}
```

```
@misc{kullm,
  author = {NLP & AI Lab and Human-Inspired AI research},
  title = {KULLM: Korea University Large Language Model Project},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/nlpai-lab/kullm}},
}
```